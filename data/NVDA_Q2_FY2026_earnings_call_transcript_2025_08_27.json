{
  "symbol": "NVDA",
  "fiscal_year": 2026,
  "fiscal_quarter": "Q2",
  "call_date": "2025-08-27",
  "source": "Investing.com / company call",
  "transcript": "Sarah, Conference Operator: Good afternoon. My name is Sarah, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA’s Second Quarter Fiscal 2026 Financial Results Conference Call. All lines have been placed on mute to prevent any background noise. After the speakers’ remarks, there will be a question and answer session. Toshiya Hari, you may begin your conference.\n\nToshiya Hari, Moderator/Investor Relations, NVIDIA: Thank you. Good afternoon, everyone, and welcome to NVIDIA’s conference call for the second quarter of fiscal 2026. With me today from NVIDIA are Jensen Huang, president and chief executive officer, and Colette Kress, executive vice president and chief financial officer. I’d like to remind you that our call is being webcast live on NVIDIA’s Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the third quarter of fiscal 2026.\n\nThe content of today’s call is NVIDIA’s property. It can’t be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today’s earnings release, our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission.\n\nAll our statements are made as of today, August 27, 2025, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\n\nColette Kress, Executive Vice President and Chief Financial Officer, NVIDIA: Thank you, Toshiya. We delivered another record quarter while navigating what continues to be a dynamic external environment. Total revenue was $46.7 billion, exceeding our outlook as we grew sequentially across all market platforms. Data center revenue grew 56% year over year. Data center revenue also grew sequentially despite the $4 billion decline in H20 revenue.\n\nNVIDIA’s Blackwell platform reached record levels, growing sequentially by 17%. We began production shipments of GB300 in Q2. Our full-stack AI solutions for cloud service providers, neo-clouds, enterprises, and sovereigns are all contributing to our growth. We are at the beginning of an industrial revolution that will transform every industry. We see $3 trillion to $4 trillion in AI infrastructure spend by the end of the decade.\n\nThe scale and scope of these buildouts present significant long-term growth opportunities for NVIDIA. The GB200 NVL system is seeing widespread adoption with deployments at CSPs and consumer internet companies. Lighthouse model builders, including OpenAI, Meta, and Mistral, are using the GB200 NVL72 at data center scale for both training next-generation models and serving inference models in production.\n\nThe new Blackwell Ultra platform has also had a strong quarter, generating tens of billions in revenue. The transition to the GB300 has been seamless for major cloud service providers due to its shared architecture, software, and physical footprint with the GB200, enabling them to build and deploy GB300 racks with ease. The transition to the new GB300 rack-based architecture has been seamless. Factory builds in late July and early August were successfully converted to support the GB300 ramp, and today full production is underway.\n\nThe current run rate is back at full speed, producing approximately 1,000 racks per week. This output is expected to accelerate even further throughout the third quarter as additional capacity comes online. We expect widespread market availability in the second half of the year as CoreWeave prepares to bring their GB300 instance to market as they are already seeing 10x more inference performance on reasoning models compared to H100.\n\nCompared to the previous Hopper generation, GB300 and NVL72 AI factories promise a 10x improvement in token-per-watt energy efficiency, which translates to revenues as data centers are power-limited. The chips of the Rubin platform are in fab: the Vera CPU, Rubin GPU, CX9 SuperNIC, NVLink 144 scale-up switch, Spectrum X scale-out and scale-across switch, and the silicon photonics processor. Rubin remains on schedule for volume production next year.\n\nRubin will be our third-generation NVLink rack-scale AI supercomputer with a mature and full-scale supply chain. This keeps us on track with our pace of an annual product cadence and continuous innovation across compute, networking, systems, and software.\n\nIn late July, the U.S. government began reviewing licenses for sales of H20 to China customers. While a select number of our China-based customers have received licenses over the past few weeks, we have not shipped any H20 based on those licenses. U.S. government officials have expressed an expectation that the U.S. government will receive 15% of the revenue generated from licensed H20 sales. But to date, the U.S. government has not published a regulation codifying such requirement.\n\nWe have not included H20 in our Q3 outlook as we continue to work through geopolitical issues. If geopolitical issues subside, we could ship $2 billion to $5 billion in H20 revenue in Q3. And if we add more orders, we can bill more. We continue to advocate for the U.S. government to approve Blackwell for China. Our products are designed and sold for beneficial commercial use, and every licensed sale we make will benefit the U.S. economy and U.S. leadership.\n\nIn highly competitive markets, we want to win the support of every developer. America’s AI technology stack can be the world’s standard if we race and compete globally. Notably in the quarter was an increase in Hopper and H200 shipments. We also sold approximately $650 million of H20 in Q2 to an unrestricted customer outside of China.\n\nThe sequential increase in Hopper demand indicates the breadth of data center workloads that run on accelerated computing and the power of CUDA libraries and full-stack optimizations, which continuously enhance the performance and economic value of our platform. As we continue to deliver both Hopper and Blackwell GPUs, we are focusing on meeting the soaring global demand. This growth is fueled by capital expenditures from the cloud to enterprises, which are on track to invest $600 billion in data center infrastructure and compute this calendar year alone, nearly doubling in two years.\n\nWe expect annual AI infrastructure investments to continue growing, driven by several factors: reasoning and agentic AI requiring orders of magnitude more training and inference compute, global buildouts for sovereign AI, enterprise AI adoption, and the arrival of physical AI and robotics. Blackwell has set the benchmark as the new standard for AI inference performance. The market for AI inference is expanding rapidly with reasoning and agentic AI gaining traction across industries.\n\nBlackwell’s rack-scale NVLink and CUDA full-stack architecture addresses this by redefining the economics of inference. New NVFP4 4-bit precision and NVLink 72 on the GB300 platform deliver a 50x increase in energy efficiency per token compared to Hopper, enabling companies to monetize their compute at unprecedented scale. For instance, a $3 million investment in GB200 infrastructure can generate $30 million in token revenue, a 10x return.\n\nNVIDIA software innovation, combined with the strength of our developer ecosystem, has already improved Blackwell’s performance by more than 2x since its launch. Advances in CUDA, TensorRT-LLM, and Dynamo are unlocking maximum efficiency. CUDA library contributions from the open-source community along with NVIDIA’s open libraries and frameworks are now integrated into millions of workflows. This powerful flywheel of collaborative innovation between NVIDIA and global community contributions strengthens NVIDIA’s performance leadership.\n\nNVIDIA is a top contributor to open AI models, data, and software. Blackwell has introduced a groundbreaking numerical approach to large language model pretraining. Using NVFP4, computations on the GB300 can now achieve 7x faster training than the H100, which uses FP8. This innovation delivers the accuracy of 16-bit precision with the speed and efficiency of 4-bit, setting a new standard for AI factory efficiency and scalability.\n\nThe AI industry is quickly adopting this revolutionary technology, with major players such as AWS, Google Cloud, Microsoft Azure, and OpenAI, as well as Cohere, Mistral, Kimi AI, Perplexity, Reflection, and Runway, already embracing it.\n\nNVIDIA’s performance leadership was further validated in the latest MLPerf training benchmarks, where the GB200 delivered a clean sweep. Be on the lookout for the upcoming MLPerf inference results in September, which will include benchmarks based on the Blackwell Ultra.\n\nNVIDIA RTX Pro Servers are in full production for the world’s system makers. These are air-cooled, PCIe-based systems integrated seamlessly into standard IT environments and run traditional enterprise IT applications as well as the most advanced agentic and physical AI applications. Nearly 90 companies, including many global leaders, are already adopting RTX Pro Servers. Hitachi uses them for real-time simulation and digital twins, Lilly for drug discovery, Hyundai for factory design and AV validation, and Disney for immersive storytelling. As enterprises modernize data centers, RTX Pro Servers are poised to become a multibillion-dollar product line.\n\nSovereign AI is on the rise as nations’ ability to develop their own AI using domestic infrastructure, data, and talent presents a significant opportunity for NVIDIA. NVIDIA is at the forefront of landmark initiatives across the U.K. and Europe. The European Union plans to invest €20 billion to establish 20 AI factories across France, Germany, Italy, and Spain, including five gigafactories to increase its AI compute infrastructure by tenfold.\n\nIn the U.K., the Isambard AI supercomputer powered by NVIDIA was unveiled as the country’s most powerful AI system, delivering 21 exaflops of AI performance to accelerate breakthroughs in fields like drug discovery and climate modeling. We are on track to achieve over $20 billion in sovereign AI revenue this year, more than double that of last year.\n\nNetworking delivered record revenue of $7.3 billion, and escalating demands of AI compute clusters necessitate high-efficiency and low-latency networking. This represents a 46% sequential and 98% year-on-year increase with strong demand across Spectrum X Ethernet, InfiniBand, and NVLink. Our Spectrum X enhanced Ethernet solutions provide the highest throughput and lowest latency network for Ethernet AI workloads. Spectrum X Ethernet delivered double-digit sequential and year-over-year growth with annualized revenue exceeding $10 billion.\n\nAt Hot Chips, we introduced Spectrum XGS Ethernet, a technology designed to unify disparate data centers into gigascale AI super factories. CoreWeave is an initial adopter of the solution, which is projected to double GPU-to-GPU communication speed. InfiniBand revenue nearly doubled sequentially, fueled by the adoption of XDR technology, which provides double the bandwidth over its predecessor, especially valuable for the model builders.\n\nThe world’s fastest switch, NVLink, with 14x the bandwidth of PCIe Gen5, delivered strong growth as customers deployed Grace Blackwell NVLink rack-scale systems. The positive reception to NVLink Fusion, which allows semi-custom AI infrastructure, has been widespread. Japan’s upcoming Fugaku NEXT will integrate Fujitsu’s CPUs with our architecture via NVLink Fusion. It will run a range of workloads, including AI, supercomputing, and quantum computing.\n\nFugaku NEXT joins a rapidly expanding list of leading quantum supercomputing and research centers running on NVIDIA’s CUDA-Q quantum platform, including Jülich, AIST, NNF, and NERSC, supported by over 300 ecosystem partners, including AWS, Google Quantum AI, Quantinuum, QEra, and PsiQuantum.\n\nJetson Thor, our new robotics computing platform, is now available. Thor delivers an order of magnitude greater AI performance and energy efficiency than NVIDIA AGX Orin. It runs the latest generative and reasoning AI models at the edge in real time, enabling state-of-the-art robotics.\n\nAdoption of NVIDIA’s robotics full-stack platform is growing at a rapid rate, with over 2 million developers and 1,000+ hardware, software, application, and sensor partners taking our platform to market. Leading enterprises across industries have adopted Thor, including Agility Robotics, Amazon Robotics, Boston Dynamics, Caterpillar, Figure, Hexagon, Medtronic, and Meta. Robotic applications require exponentially more compute on the device and in infrastructure, representing a significant long-term demand driver for our data center platform.\n\nNVIDIA Omniverse with Cosmos is our data center physical AI digital twin platform built for development of robots and robotic systems. This quarter, we announced a major expansion of our partnership with Siemens to enable AI-automated factories. Leading European robotics companies, including Agile Robots, Neura Robotics, and Universal Robots, are building their latest innovations with the Omniverse platform.\n\nTransitioning to a quick summary of our revenue by geography: China declined on a sequential basis to a low single-digit percentage of data center revenue. Note, our Q3 outlook does not include H20 shipments to China customers. Singapore revenue represented 22% of second quarter’s billed revenue as customers have centralized their invoicing in Singapore. Over 99% of data center compute revenue billed to Singapore was for U.S.-based customers.\n\nOur gaming revenue was a record $4.3 billion, a 14% sequential increase and a 49% jump year-on-year. This was driven by the ramp of Blackwell GeForce GPUs as strong sales continued and we increased supply availability. This quarter, we shipped GeForce RTX 5060 desktop GPU. It brings double the performance along with advanced ray tracing, neural rendering, and AI-powered DLSS 4 gameplay to millions of gamers worldwide.\n\nBlackwell is coming to GeForce NOW in September. This is GeForce NOW’s most significant upgrade, offering RTX 5080-class performance, minimal latency, and 5K resolution at 120 frames per second. We are also doubling the GeForce NOW catalog to over 4,500 titles, the largest library of any cloud gaming service.\n\nFor AI enthusiasts, on-device AI performs best on RTX GPUs. We partnered with OpenAI to optimize their open-source GPT models for high-quality, fast, and efficient inference on millions of RTX-enabled Windows devices. With the RTX platform stack, Windows developers can create AI applications designed to run on the world’s largest AI PC user base.\n\nProfessional visualization revenue reached $601 million, a 32% year-on-year increase. Growth was driven by adoption of the high-end RTX workstation GPUs and AI-powered workloads like design, simulation, and prototyping. Key customers are leveraging our solutions to accelerate their operations. Activision Blizzard uses RTX workstations to enhance creative workflows, while robotics innovator Figure AI powers its humanoid robots with RTX embedded GPUs.\n\nAutomotive revenue, which includes only in-car compute revenue, was $586 million, up 69% year-on-year, primarily driven by self-driving solutions. We have begun shipments of NVIDIA Thor SoC, the successor to Orin. Thor’s arrival coincides with the industry’s accelerating shift to vision-language model architectures, generative AI, and higher levels of autonomy. Thor is the most successful robotics and AV computer we’ve ever created. Thor will power our full-stack DRIVE AV software platform, now in production, opening up billions in new revenue opportunities for NVIDIA while improving vehicle safety and autonomy.\n\nNow moving to the rest of our P&L. GAAP gross margin was 72.4%, and non-GAAP gross margin was 72.7%. These figures include a $180 million, or 40 basis-point, benefit from releasing previously reserved H20 inventory. Excluding this benefit, non-GAAP gross margins would have been 72.3%, still exceeding our outlook.\n\nGAAP operating expenses rose 8% and 6% on a non-GAAP basis sequentially. This increase was driven by higher compute and infrastructure costs as well as higher compensation and benefit costs. To support the ramp of Blackwell and Blackwell Ultra, inventory increased sequentially from $11 billion to $15 billion in Q2. While we prioritize funding our growth and strategic initiatives, in Q2 we returned $10 billion to shareholders through share repurchases and cash dividends.\n\nOur board of directors recently approved a $60 billion share repurchase authorization to add to our remaining $14.7 billion of authorization at the end of Q2.\n\nOkay. Let me turn to the outlook for the third quarter. Total revenue is expected to be $54 billion, plus or minus 2%. This represents over $7 billion in sequential growth. Again, we do not assume any H20 shipments to China customers in our outlook. GAAP and non-GAAP gross margins are expected to be 73.3% and 73.5%, respectively, plus or minus 50 basis points. We continue to expect to exit the year with non-GAAP gross margins in the mid-70s.\n\nGAAP and non-GAAP operating expenses are expected to be approximately $5.9 billion and $4.2 billion, respectively. For the full year, we expect operating expenses to grow in the high-30s range year-over-year, up from our prior expectations of the mid-30s. We are accelerating investments in the business to address the magnitude of growth opportunities that lie ahead.\n\nGAAP and non-GAAP other income and expenses are expected to be an income of approximately $500 million, excluding gains and losses from non-marketable and publicly held equity securities. GAAP and non-GAAP tax rates are expected to be 16.5%, plus or minus 1%, excluding any discrete items. Further financial data are included in the CFO commentary and other information available on our website.\n\nIn closing, let me highlight upcoming events for the financial community. We will be at the Goldman Sachs Technology Conference on September 8 in San Francisco. Our annual NDR will commence in October. GTC Data Center begins on October 27 with Jensen’s keynote scheduled for the 28th. We look forward to seeing you at these events. Our earnings call to discuss the results of our third quarter of fiscal 2026 is scheduled for November 19.\n\nWe will now open the call for questions. Operator, would you please poll for questions?\n\nSarah, Conference Operator: Thank you. Your first question comes from C.J. Muse with Cantor Fitzgerald. Your line is open.\n\nC.J. Muse, Analyst, Cantor Fitzgerald: Yes, good afternoon. Thank you for taking the question. I guess with wafer-in to rack-out lead times of 12 months, you confirmed on the call today that Rubin is on track for ramp in the second half. And obviously many of these investments are multiyear projects contingent upon power, cooling, etc. I was hoping perhaps you could take a high-level view and speak to your vision for growth into 2026. And as part of that, if you could comment between networking and data center, that would be very helpful. Thank you.\n\nJensen Huang, President and Chief Executive Officer, NVIDIA: Yeah. Thanks, C.J. At the highest level, the growth drivers would be the evolution and introduction, if you will, of reasoning and agentic AI. Where chatbots used to be one shot — you give it a prompt and it would generate the answer — now the AI does research. It thinks, it plans, and it might use tools. So it’s called long thinking, and the longer it thinks, often the better the answers.\n\nThe amount of computation necessary for one-shot versus reasoning and agentic AI models could be 100x, 1,000x, and potentially even more as the amount of research and reading and comprehension that it goes off to do increases. So the amount of computation that has resulted from agentic AI has grown tremendously, and of course the effectiveness has also grown tremendously.\n\nBecause of agentic AI, the amount of hallucination has dropped significantly. You can now use tools and perform tasks. Enterprises have opened up. As a result of agentic AI and vision-language models, we now are seeing a breakthrough in physical AI — in robotics and autonomous systems. So in the last year, AI has made tremendous progress, and agentic, reasoning systems are completely revolutionary.\n\nWe built the Blackwell NVLink 72 system, a rack-scale computing system, for this moment. We’ve been working on it for several years. This last year, we transitioned from NVLink 8, which is a node-scale computing architecture where each node is a computer, to now NVLink 72 where each rack is a computer. That disaggregation of NVLink 72 into a rack-scale system was extremely hard to do, but the results are extraordinary.\n\nWe’re seeing orders of magnitude speed-up and therefore energy efficiency and cost-effectiveness of token generation because of NVLink 72. Over the next several years, with Blackwell, Rubin, and follow-ons, we are going to scale into what is effectively a $3 trillion to $4 trillion AI infrastructure opportunity. The last couple of years, you’ve seen that CapEx has grown — in just the top four CSPs it has doubled and grown to about $600 billion. So we’re at the beginning of this build-out, and AI technology advances have really enabled AI to adopt and solve problems across many different industries.\n\n[Q&A continues with additional analysts and management exchanges covering China, ASIC competition, margin expectations, power and supply constraints, AI infrastructure growth assumptions, and NVIDIA’s positioning across GPUs, networking, and full-stack AI platforms.]\n\nSarah, Conference Operator: This concludes today’s conference call. You may now disconnect."
}
