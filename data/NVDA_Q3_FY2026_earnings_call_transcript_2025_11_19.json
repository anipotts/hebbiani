{
  "symbol": "NVDA",
  "fiscal_year": 2026,
  "fiscal_quarter": "Q3",
  "call_date": "2025-11-19",
  "source": "Investing.com / company call",
  "transcript": "Sarah, Conference Operator: Good afternoon. My name is Sarah, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA’s third quarter earnings call. All lines have been placed on mute to prevent any background noise. After the speakers’ remarks, there will be a question-and-answer session. If you would like to ask a question during this time, simply press star, followed by the number one on your telephone keypad. If you would like to withdraw your question, press star one again. Thank you. Toshiya Hari, you may begin your conference.\n\nToshiya Hari, Investor Relations, NVIDIA: Thank you. Good afternoon, everyone, and welcome to NVIDIA’s conference call for the third quarter of fiscal 2026. With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer, and Colette Kress, Executive Vice President and Chief Financial Officer. I’d like to remind you that our call is being webcast live on NVIDIA’s Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter of fiscal 2026.\n\nThe content of today’s call is NVIDIA’s property. It can’t be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially.\n\nFor a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today’s earnings release, our most recent Forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, November 19, 2025, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements.\n\nDuring this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\n\nColette Kress, Executive Vice President and Chief Financial Officer, NVIDIA: Thank you, Toshiya. We delivered another outstanding quarter with revenue of $57 billion, up 62% year-over-year, and a record sequential revenue growth of $10 billion, or 22%. Our customers continue to lean into three platform shifts, fueling exponential growth for accelerated computing, powerful AI models, and agentic applications. Yet, we are still in the early innings of these transitions that will impact work across every industry.\n\nWe currently have visibility to $500 billion in Blackwell and Rubin revenue from the start of this year through the end of calendar year 2026. By executing our annual product cadence and extending our performance leadership through full-stack design, we believe NVIDIA will be the superior choice for the $3 trillion–$4 trillion in annual AI infrastructure build we estimate by the end of the decade. Demand for AI infrastructure continues to exceed our expectations.\n\nThe clouds are sold out, and our GPU installed base, both new and previous generations including Blackwell, Hopper, and Ampere, is fully utilized. Record Q3 data center revenue of $51 billion increased 66% year-over-year, a significant feat at our scale. Compute grew 56% year-over-year, driven primarily by the GB300 ramp, while networking more than doubled given the onset of NVLink scale-up and robust double-digit growth across Spectrum X Ethernet and Quantum X InfiniBand.\n\nThe world’s hyperscalers, a trillion-dollar industry, are transforming search, recommendations, and content understanding from classical machine learning to generative AI. NVIDIA CUDA excels at both and is the ideal platform for this transition, driving infrastructure investment measured in hundreds of billions of dollars. At Meta, AI recommendation systems are delivering higher quality and more relevant content, leading to more time spent on apps such as Facebook and Threads.\n\nAnalyst expectations for the top CSPs and hyperscalers’ 2026 aggregate CapEx have continued to increase and now sit roughly at $600 billion, more than $200 billion higher relative to the start of the year. We see the transition to accelerated computing and generative AI across current hyperscale workloads contributing toward roughly half of our long-term opportunity.\n\nAnother growth pillar is the ongoing increase in compute spend driven by foundation model builders such as Anthropic, Mistral, OpenAI, Reflection, Safe Superintelligence, Thinking Machines Lab, and xAI, all scaling compute aggressively to scale intelligence. The three scaling laws — pre-training, post-training, and inference — remain intact. In fact, we see a positive virtuous cycle emerging whereby the three scaling laws and access to compute are generating better intelligence and, in turn, increasing adoption and profits.\n\nOpenAI recently shared that their weekly user base has grown to 800 million, enterprise customers have increased to 1 million, and that their gross margins were healthy. Anthropic recently reported that its annualized run-rate revenue has reached $7 billion as of last month, up from $1 billion at the start of the year.\n\nWe are also witnessing a proliferation of agentic AI across various industries and tasks. Companies such as Cursor, Anthropic, Open Evidence, Epic, and Abridge are experiencing a surge in user growth as they supercharge the existing workforce, delivering unquestionable ROI for coders and healthcare professionals. The world’s most important enterprise software platforms like ServiceNow, CrowdStrike, and SAP are integrating NVIDIA’s accelerated computing and AI stack. Our new partner, Palantir, is supercharging the incredibly popular Ontology platform with NVIDIA CUDA-X libraries and AI models for the first time.\n\nPreviously, like most enterprise software platforms, Ontology ran only on CPUs. Lowe’s is leveraging the platform to build supply chain agility, reducing costs and improving customer satisfaction. Enterprises broadly are leveraging AI to boost productivity, increase efficiency, and reduce costs. RBC is leveraging agentic AI to drive significant analyst productivity, slashing report generation time from hours to minutes. AI and digital twins are helping Unilever accelerate content creation by 2x and cut costs by 50%. Salesforce’s engineering team has seen at least a 30% productivity increase in new code development after adopting Cursor.\n\nThis past quarter, we announced AI factory and infrastructure projects amounting to an aggregate of 5 million GPUs. This demand spans every market: CSPs, sovereigns, model builders, enterprises, and supercomputing centers, and includes multiple landmark buildouts.\n\nxAI’s Colossus 2, the world’s first gigawatt-scale data center; Lilly’s AI factory for drug discovery, the pharmaceutical industry’s most powerful data center. Just today, AWS and Humane expanded their partnership, including the deployment of up to 150,000 AI accelerators, including our GB300. xAI and Humane also announced a partnership in which the two will jointly develop a network of world-class GPU data centers anchored by the flagship 500-megawatt facility.\n\nBlackwell gained further momentum in Q3 as GB300 crossed over GB200 and contributed roughly two-thirds of total Blackwell revenue. The transition to GB300 has been seamless, with production shipments to the major cloud service providers, hyperscalers, and GPU clouds, and is already driving their growth.\n\nThe Hopper platform, in its 13th quarter since inception, recorded approximately $2 billion in revenue in Q3. H20 sales were approximately $50 million. Sizable purchase orders never materialized in the quarter due to geopolitical issues and the increasingly competitive market in China. While we were disappointed in the current state that prevents us from shipping more competitive data center compute products to China, we are committed to continued engagement with the U.S. and China governments and will continue to advocate for America’s ability to compete around the world.\n\nTo establish a sustainable leadership position in AI computing, America must win the support of every developer and be the platform of choice for every commercial business, including those in China.\n\nThe Rubin platform is on track to ramp in the second half of 2026. Powered by seven chips, the Vera Rubin platform will once again deliver an X-factor improvement in performance relative to Blackwell. We have received silicon back from our supply chain partners and are happy to report that NVIDIA teams across the world are executing the bring-up beautifully.\n\nRubin is our third-generation rack-scale system — it substantially redefines manufacturability while remaining compatible with Grace Blackwell. Our supply chain, data center ecosystem, and cloud partners have now mastered the build-to-installation process of NVIDIA’s rack architecture. Our ecosystem will be ready for a fast Rubin ramp.\n\nOur annual X-factor performance leap increases performance per dollar while driving down computing costs for our customers. The long useful life of NVIDIA’s CUDA GPUs is a significant TCO advantage over accelerators. CUDA’s compatibility and our massive installed base extend the life of NVIDIA systems well beyond their original estimated useful life.\n\nFor more than two decades, we have optimized the CUDA ecosystem, improving existing workloads, accelerating new ones, and increasing throughput with every software release. Most accelerators without CUDA and NVIDIA’s time-tested and versatile architecture become obsolete within a few years as model technologies evolve. Thanks to CUDA, the A100 GPUs we shipped six years ago are still running at full utilization today, powered by a vastly improved software stack.\n\nWe have evolved over the past 25 years from a gaming GPU company to an AI data center infrastructure company. Our ability to innovate across the CPU, the GPU, networking, and software, and ultimately drive down cost per token, is unmatched across the industry. Our networking business, purpose-built for AI and now the largest in the world, generated revenue of $8.2 billion, up 162% year-over-year, with NVLink, InfiniBand, and Spectrum X Ethernet all contributing to growth.\n\nWe are winning in data center networking as the majority of AI deployments now include our switches, with Ethernet GPU attach rates roughly on par with InfiniBand. Meta, Microsoft, Oracle, and xAI are building gigawatt AI factories with Spectrum X Ethernet switches, and each will run its operating system of choice, highlighting the flexibility and openness of our platform.\n\nWe recently introduced Spectrum XGS, a scale-across technology that enables gigascale AI factories. NVIDIA is the only company with AI scale-up, scale-out, and scale-across platforms, reinforcing our unique position in the market as the AI infrastructure provider.\n\nCustomer interest in NVLink Fusion continues to grow. We announced a strategic collaboration with Fujitsu in October, where we will integrate Fujitsu’s CPUs and NVIDIA GPUs via NVLink Fusion, connecting our large ecosystems. We also announced a collaboration with Intel to develop multiple generations of custom data center and PC products, connecting NVIDIA and Intel’s ecosystems using NVLink.\n\nThis week at Supercomputing 25, Arm announced that it will be integrating NVLink IP for customers to build CPU SoCs that connect with NVIDIA. Currently on its fifth generation, NVLink is the only proven scale-up technology available on the market today.\n\nIn the latest MLPerf training results, Blackwell Ultra delivered 5x faster time-to-train than Hopper. NVIDIA swept every benchmark. Notably, NVIDIA is the only training platform to leverage bridge FP4 while meeting MLPerf’s strict accuracy standards. In SemiAnalysis’ inference MAX benchmark, Blackwell achieved the highest performance and lowest total cost of ownership across every model and use case.\n\nParticularly important is Blackwell’s NVLink performance on mixture-of-experts models, the architecture for the world’s most popular reasoning models. On DeepSeek R1, Blackwell delivered 10x higher performance-per-watt and 10x lower cost-per-token versus H200 — a huge generational leap fueled by our extreme co-design approach.\n\nNVIDIA Dynamo, an open-source, low-latency modular inference framework, has now been adopted by every major cloud service provider. Leveraging Dynamo’s enablement and disaggregated inference, the resulting increase in performance of complex AI models such as MoE models across AWS, Google Cloud, Microsoft Azure, and OCI has boosted AI inference performance for enterprise cloud customers.\n\nWe are working on a strategic partnership with OpenAI focused on helping them build and deploy at least 10 gigawatts of AI data centers. In addition, we have the opportunity to invest in the company. We serve OpenAI through their cloud partners — Microsoft Azure, OCI, and CoreWeave — and will continue to do so for the foreseeable future. As they continue to scale, we are delighted to support the company to add self-build infrastructure. We are working toward a definitive agreement and are excited to support OpenAI’s growth.\n\nYesterday, we celebrated an announcement with Anthropic. For the first time, Anthropic is adopting NVIDIA, and we are establishing a deep technology partnership to support Anthropic’s fast growth. We will collaborate to optimize Anthropic models for CUDA and deliver the best possible performance, efficiency, and TCO. We will also optimize future NVIDIA architectures for Anthropic workloads.\n\nAnthropic’s compute commitment initially includes up to 1 gigawatt of compute capacity with Grace Blackwell and Vera Rubin systems. Our strategic investments in Anthropic, Mistral, OpenAI, Reflection, Thinking Machines, and others represent partnerships that grow the NVIDIA CUDA AI ecosystem and enable every model to run optimally on NVIDIA everywhere. We will continue to invest strategically while preserving our disciplined approach to cash-flow management.\n\nPhysical AI is already a multi-billion-dollar business addressing a multi-trillion-dollar opportunity and the next leg of growth for NVIDIA. Leading U.S. manufacturers and robotics innovators are leveraging NVIDIA’s three-computer architecture to train on NVIDIA, test on Omniverse compute, and deploy real-world AI on Jetson robotic computers.\n\nPTC and Siemens introduced new services that bring Omniverse-powered digital twin workflows to their extensive installed base of customers. Companies including Belden, Caterpillar, Foxconn, Lucid Motors, Toyota, TSMC, and Wistron are building Omniverse digital twin factories to accelerate AI-driven manufacturing and automation. Agility Robotics, Amazon Robotics, Figure, and SKILLED.AI are building on our platform, tapping offerings such as NVIDIA Cosmos world foundation models for development, Omniverse for simulation and validation, and Jetson to power next-generation intelligent robots.\n\nWe remain focused on building resiliency and redundancy in our global supply chain. Last month, in partnership with TSMC, we celebrated the first Blackwell wafer produced on U.S. soil. We will continue to work with Foxconn, Wistron, Amkor, SPIL, and others to grow our presence in the U.S. over the next four years.\n\nGaming revenue was $4.3 billion, up 30% year-on-year, driven by strong demand as Blackwell momentum continued. End-market sell-through remains robust, and channel inventories are at normal levels heading into the holiday season. Steam recently broke its concurrent user record with 42 million gamers, while thousands of fans packed the GeForce Gamer Festival in South Korea to celebrate 25 years of GeForce.\n\nNVIDIA Pro Visualization has evolved into computers for engineers and developers, whether for graphics or AI. Professional visualization revenue was $760 million, up 56% year-over-year — another record. Growth was driven by DGX Spark, the world’s smallest AI supercomputer built on a small configuration of Grace Blackwell.\n\nAutomotive revenue was $592 million, up 32% year-over-year, primarily driven by self-driving solutions. We are partnering with Uber to scale the world’s largest Level-4-ready autonomous fleet, built on the new NVIDIA Hyperion L4 Robotaxi reference architecture.\n\nMoving to the rest of the P&L: GAAP gross margins were 73.4%, and non-GAAP gross margins were 73.6%, exceeding our outlook. Gross margins increased sequentially due to our data center mix, improved cycle time, and cost structure. GAAP operating expenses were up 8% sequentially and up 11% on a non-GAAP basis. The growth was driven by infrastructure compute as well as higher compensation and benefits and engineering development costs.\n\nThe non-GAAP effective tax rate for the third quarter was just over 17%, higher than our guidance of 16.5% due to strong U.S. revenue. On our balance sheet, inventory grew 32% quarter-over-quarter, while supply commitments increased 63% sequentially. We are preparing for significant growth ahead and feel good about our ability to execute against our opportunity set.\n\nOkay, let me turn to the outlook for the fourth quarter. Total revenue is expected to be $65 billion, plus or minus 2%. At the midpoint, our outlook implies 14% sequential growth driven by continued momentum in the Blackwell architecture. Consistent with last quarter, we are not assuming any data center compute revenue from China.\n\nGAAP and non-GAAP gross margins are expected to be 74.8% and 75%, respectively, plus or minus 50 basis points. Looking ahead to fiscal year 2027, input costs are on the rise, but we are working to hold gross margins in the mid-70s. GAAP and non-GAAP operating expenses are expected to be approximately $6.7 billion and $5 billion, respectively.\n\nGAAP and non-GAAP other income and expenses are expected to be an income of approximately $500 million, excluding gains and losses from non-marketable and publicly held equity securities. GAAP and non-GAAP tax rates are expected to be 17%, plus or minus 1%, excluding any discrete items.\n\nAt this time, let me turn the call over to Jensen for him to say a few words.\n\nJensen Huang, President and Chief Executive Officer, NVIDIA: Thanks, Colette. There has been a lot of talk about an AI bubble. From our vantage point, we see something very different. As a reminder, NVIDIA is unlike any other accelerator. We excel at every phase of AI, from pre-training and post-training to inference.\n\nWith our two-decade investment in CUDA-X acceleration libraries, we are also exceptional at science and engineering simulations, computer graphics, and structured data processing to classical machine learning.\n\nThe world is undergoing three massive platform shifts at once, for the first time since the dawn of Moore’s Law. NVIDIA is uniquely addressing each of the three transformations.\n\nThe first transition is from CPU-based general-purpose computing to GPU-accelerated computing as Moore’s Law slows. The world has a massive investment in non-AI software — from data processing to science and engineering simulations — representing hundreds of billions of dollars in compute cloud-computing spend each year. Many of these applications, which ran once exclusively on CPUs, are now rapidly shifting to CUDA GPUs. Accelerated computing has reached a tipping point.\n\nSecondly, AI has also reached a tipping point and is transforming existing applications while enabling entirely new ones. For existing applications, generative AI is replacing classical machine learning in search ranking, recommender systems, ad targeting, click-through prediction, and content moderation — the very foundations of hyperscale infrastructure.\n\nMeta’s Gem, a foundation model for ad recommendations trained on large-scale GPU clusters, exemplifies this shift. In Q2, Meta reported over a 5% increase in ad conversions on Instagram and a 3% gain on the Facebook feed, driven by generative-AI-based Gem. Transitioning to generative AI represents substantial revenue gains for hyperscalers.\n\nNow, a new wave is rising: agentic AI systems capable of reasoning, planning, and using tools. From coding assistants like Cursor and Claude Code to radiology tools like iDoc, legal assistants like Harvey, and AI chauffeurs like Tesla FSD and Waymo, these systems mark the next frontier of computing. The fastest-growing companies in the world today — OpenAI, Anthropic, xAI, Google, Cursor, Lovable, Replit, Cognition AI, Open Evidence, Abridge, Tesla — are pioneering agentic AI.\n\nThere are three massive platform shifts. The transition to accelerated computing is foundational and necessary — essential in a post-Moore’s Law era. The transition to generative AI is transformational and necessary — supercharging existing applications and business models. The transition to agentic and physical AI will be revolutionary, giving rise to new applications, companies, products, and services.\n\nAs you consider infrastructure investments, consider these three fundamental dynamics. Each will contribute to infrastructure growth in the coming years. NVIDIA is chosen because our singular architecture enables all three transitions — for any form and modality of AI across all industries, across every phase of AI, across all of the diverse computing needs in the cloud, and also from cloud to enterprise to robots. One architecture.\n\nToshiya, back to you.\n\nToshiya Hari, Investor Relations, NVIDIA: Thank you. We will now open the call for questions. Operator, would you please poll for questions?\n\nSarah, Conference Operator: Thank you. At this time, I would like to remind everyone that in order to ask a question, press star then the number one on your telephone keypad. We’ll pause for just a moment to compile the Q&A roster. As a reminder, please limit yourself to one question.\n\nYour first question comes from Joseph Moore with Morgan Stanley. Your line is open.\n\n[Q&A with multiple analysts follows, covering Blackwell/Rubin revenue visibility, sustainability of AI CapEx, supply/demand balance, per-gigawatt economics, financing vs cash-flow funding, NVIDIA’s use of cash and ecosystem investments, the role of inference vs training, long-context workloads and CPX, behind-the-meter power constraints, and NVIDIA’s competitive advantages versus ASICs and custom accelerators.]\n\nSarah, Conference Operator: Thank you. This concludes today’s conference call. You may now disconnect."
}
